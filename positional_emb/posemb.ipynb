{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = torch.randn(10, 32, 32, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(32), torch.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = torch.meshgrid(torch.arange(5), torch.arange(4), indexing='ij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         [2, 2, 2, 2],\n",
       "         [3, 3, 3, 3],\n",
       "         [4, 4, 4, 4]]),\n",
       " tensor([[0, 1, 2, 3],\n",
       "         [0, 1, 2, 3],\n",
       "         [0, 1, 2, 3],\n",
       "         [0, 1, 2, 3],\n",
       "         [0, 1, 2, 3]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[4, 5] i64 n=20 x∈[0, 4] μ=2.000 σ=1.451\n",
       " tensor([[0, 1, 2, 3, 4],\n",
       "         [0, 1, 2, 3, 4],\n",
       "         [0, 1, 2, 3, 4],\n",
       "         [0, 1, 2, 3, 4]]),\n",
       " tensor[4, 5] i64 n=20 x∈[0, 3] μ=1.500 σ=1.147\n",
       " tensor([[0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [2, 2, 2, 2, 2],\n",
       "         [3, 3, 3, 3, 3]]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x = torch.meshgrid(torch.arange(5), torch.arange(4), indexing='xy')\n",
    "y.v, x.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = torch.arange(128 //4) / (128 // 4 -1)  # linsapce같은 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 1 / (10000 ** omega) # 점점 작아지도록 [dim//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.flatten()[:None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [20, 1] * [1, 32] => [20, 32]\n",
    "y = y.flatten()[:,None] * omega[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.flatten()[:, None] * omega[None, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_tensors as lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[20, 128] n=2560 x∈[-0.990, 1.000] μ=0.513 σ=0.487\n"
     ]
    }
   ],
   "source": [
    "lt.monkey_patch()\n",
    "\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[20, 128] n=2560 x∈[-0.990, 1.000] μ=0.513 σ=0.487\n",
      "  tensor[128] x∈[0., 1.000] μ=0.500 σ=0.502\n",
      "  tensor[128] x∈[0., 1.000] μ=0.520 σ=0.481\n",
      "  tensor[128] x∈[-0.416, 1.000] μ=0.517 σ=0.484\n",
      "  tensor[128] x∈[-0.990, 1.000] μ=0.500 σ=0.502\n",
      "  tensor[128] x∈[-0.986, 1.000] μ=0.482 σ=0.520\n",
      "  tensor[128] x∈[0., 1.000] μ=0.520 σ=0.481\n",
      "  tensor[128] x∈[1.000e-04, 1.000] μ=0.540 σ=0.458\n",
      "  tensor[128] x∈[-0.416, 1.000] μ=0.537 σ=0.461\n",
      "  tensor[128] x∈[-0.990, 1.000] μ=0.520 σ=0.481\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "print(pe.deeper(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[10, 2] n=20 x∈[-1.664, 2.294] μ=0.309 σ=0.992,\n",
       " tensor[10, 2] n=20 x∈[-1.542, 2.610] μ=0.201 σ=1.083)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(10, 4).chunk(2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 2] n=20 x∈[-2.300, 0.269] μ=-0.337 σ=0.770"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.gelu(torch.randn(10, 2)) * torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 10, 32] n=320 x∈[-3.325, 2.751] μ=2.235e-09 σ=1.002"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.layer_norm(x, x.shape[-1:], torch.ones(32), torch.zeros(32))a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 10, 32] n=320 x∈[-3.325, 2.751] μ=2.235e-09 σ=1.002"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.layer_norm(x, x.shape[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return torch.nn.functional.gelu(gate) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 33, 44, 10] n=43560 x∈[-0.893, 0.824] μ=-0.097 σ=0.208 grad AddBackward0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(LayerNorm(10),\n",
    "              nn.Linear(10, 20),\n",
    "              GEGLU(),\n",
    "              nn.Linear(10, 10)\n",
    "              )(torch.randn(3, 33, 44, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(10*4*2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Spectrogram, TimeStretch, FrequencyMasking, TimeMasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return (t, t) if not isinstance(t, tuple) else t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4], [1, 2, 3, 4])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AudioSpectrogramTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, patch_size=16, dim_head=64, heads=8,\n",
    "                 attn_dropout=0., ff_mult=4, ff_dropout=0.,spec_n_fft=128):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat, reduce, unpack, pack\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = pair(16)\n",
    "patch_input_dim = patch_size[0] * patch_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": "Expected 3 dimensions, got 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nn\u001b[39m.\u001b[39;49mSequential(Rearrange(\u001b[39m'\u001b[39;49m\u001b[39mb (h p1) (w p2) -> b h w (p1 p2)\u001b[39;49m\u001b[39m'\u001b[39;49m, p1\u001b[39m=\u001b[39;49mpatch_size[\u001b[39m0\u001b[39;49m],p2\u001b[39m=\u001b[39;49mpatch_size[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m      2\u001b[0m               nn\u001b[39m.\u001b[39;49mLayerNorm(patch_input_dim),\n\u001b[1;32m      3\u001b[0m               nn\u001b[39m.\u001b[39;49mLinear(patch_input_dim, \u001b[39m10\u001b[39;49m),\n\u001b[1;32m      4\u001b[0m               nn\u001b[39m.\u001b[39;49mLayerNorm(\u001b[39m10\u001b[39;49m))(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m10\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m32\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/einops/layers/torch.py:14\u001b[0m, in \u001b[0;36mRearrange.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_for_scriptable_torch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recipe, \u001b[39minput\u001b[39;49m, reduction_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrearrange\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/einops/_torch_specific.py:77\u001b[0m, in \u001b[0;36mapply_for_scriptable_torch\u001b[0;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_for_scriptable_torch\u001b[39m(recipe: TransformRecipe, tensor: torch\u001b[39m.\u001b[39mTensor, reduction_type: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     75\u001b[0m     backend \u001b[39m=\u001b[39m TorchJitBackend\n\u001b[1;32m     76\u001b[0m     init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> 77\u001b[0m         _reconstruct_from_shape_uncached(recipe, backend\u001b[39m.\u001b[39;49mshape(tensor))\n\u001b[1;32m     78\u001b[0m     tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mreshape(tensor, init_shapes)\n\u001b[1;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(reduced_axes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/einops/einops.py:165\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_composite_axes):\n\u001b[0;32m--> 165\u001b[0m         \u001b[39mraise\u001b[39;00m EinopsError(\u001b[39m'\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_composite_axes), \u001b[39mlen\u001b[39m(shape)))\n\u001b[1;32m    167\u001b[0m ellipsis_shape: List[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m    168\u001b[0m \u001b[39mfor\u001b[39;00m input_axis, (known_axes, unknown_axes) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_composite_axes):\n",
      "\u001b[0;31mEinopsError\u001b[0m: Expected 3 dimensions, got 4"
     ]
    }
   ],
   "source": [
    "nn.Sequential(Rearrange('b (h p1) (w p2) -> b h w (p1 p2)', p1=patch_size[0],p2=patch_size[1]),\n",
    "              nn.LayerNorm(patch_input_dim),\n",
    "              nn.Linear(patch_input_dim, 10),\n",
    "              nn.LayerNorm(10))(torch.randn(10, 2, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((10,2,3,4,5))[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2] i64 μ=1.000 σ=1.414 [2, 0]\n",
       "tensor([2, 0])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(torch.meshgrid(\n",
    "            torch.arange(4),\n",
    "            torch.arange(5)\n",
    "        , indexing = 'ij'),dim=-1)[2][0].v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.stack(torch.meshgrid(\n",
    "            torch.arange(16),\n",
    "            torch.arange(16)\n",
    "        , indexing = 'ij'),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[16, 16, 2] i64 n=512 x∈[0, 15] μ=7.500 σ=4.614"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = rearrange(grid, '... c -> (...) c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[256, 2] i64 n=512 x∈[0, 15] μ=7.500 σ=4.614"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posemb_sincos_2d(patches, temperature = 10000, dtype = torch.float32):\n",
    "    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
    "\n",
    "    y, x = torch.meshgrid(torch.arange(h, device = device), torch.arange(w, device = device), indexing = 'ij')\n",
    "    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n",
    "\n",
    "    omega = torch.arange(dim // 4, device = device) / (dim // 4 - 1)\n",
    "    omega = 1. / (temperature ** omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :] \n",
    "\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim = 1)\n",
    "    pe = pe.type(dtype)\n",
    "\n",
    "    return rearrange(pe, '(h w) d -> h w d', h = h, w = w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 16, 16, 128)\n",
    "x = x + posemb_sincos_2d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 16, 16, 128] n=327680 x∈[-4.946, 5.683] μ=0.452 σ=1.138"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearrange(x, 'b ... c -> b (...) c')[0].chans(cmap='coolwarm', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[7, 3, 1, 4] n=84 x∈[-3.091, 2.171] μ=0.123 σ=1.050"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(7, 3, 4)\n",
    "\n",
    "rearrange(x, '... i c -> ... i 1 c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0041225012c46a81f6ea3650572d975902102d8f41a1704402cdfe5d667efe52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
