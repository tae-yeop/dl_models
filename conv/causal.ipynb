{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiteam/miniconda3/envs/3dfm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.causal_padding = self.dilation[0] * (self.kernel_size[0] -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._conv_forward(F.pad(x, [self.causal_padding, 0]), self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CausalConv1d(3, 10, kernel_size=4, dilation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.causal_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10, 297])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test._conv_forward(torch.randn(7, 3, 300), test.weight, test.bias).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.5056, -0.2466, -0.1164,  1.4550,  0.1818,  0.4643,\n",
       "          0.7300, -0.6947, -0.1402,  0.0484],\n",
       "        [ 0.0000,  0.0000,  0.5896,  0.0440,  1.0827,  0.2574, -1.4395,  3.6763,\n",
       "         -0.2315,  0.0047, -0.4282, -2.1393],\n",
       "        [ 0.0000,  0.0000, -0.1773,  0.3543, -0.9901,  1.6098, -1.2676, -2.8507,\n",
       "          0.2691,  0.3769, -1.0275,  0.5519]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad(torch.randn(3, 10), [2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConvTranspose1d(nn.ConvTranspose1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.causal_padding = self.dilation[0] * (self.kernel_size[0] - 1) + self.output_padding[0] + 1 - self.stride[0]\n",
    "    \n",
    "    def forward(self, x, output_size=None):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose1d')\n",
    "\n",
    "        assert isinstance(self.padding, tuple)\n",
    "        output_padding = self._output_padding(\n",
    "            x, output_size, self.stride, self.padding, self.kernel_size, self.dilation)\n",
    "        return F.conv_transpose1d(\n",
    "            x, self.weight, self.bias, self.stride, self.padding,\n",
    "            output_padding, self.groups, self.dilation)[...,:-self.causal_padding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CausalConvTranspose1d(1, 3, 3)\n",
    "test.padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"int\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m3\u001b[39;49m, \u001b[39m10\u001b[39;49m), output_size\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [14], line 11\u001b[0m, in \u001b[0;36mCausalConvTranspose1d.forward\u001b[0;34m(self, x, output_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOnly `zeros` padding mode is supported for ConvTranspose1d\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mtuple\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_padding(\n\u001b[1;32m     12\u001b[0m     x, output_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv_transpose1d(\n\u001b[1;32m     14\u001b[0m     x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding,\n\u001b[1;32m     15\u001b[0m     output_padding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,:\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal_padding]\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/nn/modules/conv.py:632\u001b[0m, in \u001b[0;36m_ConvTransposeNd._output_padding\u001b[0;34m(self, input, output_size, stride, padding, kernel_size, num_spatial_dims, dilation)\u001b[0m\n\u001b[1;32m    630\u001b[0m     ret \u001b[39m=\u001b[39m _single(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_padding)  \u001b[39m# converting to list if was not already\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     has_batch_dim \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m num_spatial_dims \u001b[39m+\u001b[39;49m \u001b[39m2\u001b[39;49m\n\u001b[1;32m    633\u001b[0m     num_non_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m has_batch_dim \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    634\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(output_size) \u001b[39m==\u001b[39m num_non_spatial_dims \u001b[39m+\u001b[39m num_spatial_dims:\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"int\") to tuple"
     ]
    }
   ],
   "source": [
    "test(torch.randn(3, 10), output_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CausalConvTranspose1d(in_channels=2*3,\n",
    "                               out_channels=3,\n",
    "                               kernel_size=2*1, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch.randn(6, 10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            CausalConv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                      kernel_size=7, dilation=dilation),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                      kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            CausalConvTranspose1d(in_channels=2*out_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=2*stride, stride=stride),\n",
    "            nn.ELU(),\n",
    "            ResidualUnit(in_channels=out_channels, out_channels=out_channels,\n",
    "                         dilation=1),\n",
    "            nn.ELU(),\n",
    "            ResidualUnit(in_channels=out_channels, out_channels=out_channels,\n",
    "                         dilation=3),\n",
    "            nn.ELU(),\n",
    "            ResidualUnit(in_channels=out_channels, out_channels=out_channels,\n",
    "                         dilation=9),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            CausalConv1d(in_channels=D, out_channels=16*C, kernel_size=7),\n",
    "            nn.ELU(),\n",
    "            DecoderBlock(out_channels=8*C, stride=8),\n",
    "            nn.ELU(),\n",
    "            DecoderBlock(out_channels=4*C, stride=5),\n",
    "            nn.ELU(),\n",
    "            DecoderBlock(out_channels=2*C, stride=4),\n",
    "            nn.ELU(),\n",
    "            DecoderBlock(out_channels=C, stride=2),\n",
    "            nn.ELU(),\n",
    "            CausalConv1d(in_channels=C, out_channels=1, kernel_size=7)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(C=1, D=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(torch.randn(1, 3, 100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.nn.Parameter(torch.Tensor(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_padding(x, kernel):\n",
    "    if kernel.shape[-1] % 2 == 0:\n",
    "        kernel = F.pad(kernel, [1,0], value=0.0)\n",
    "\n",
    "    x = F.pad(x, [kernel.shape[-1]-1, 0], value=0.0)\n",
    "    return x, kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_conv(x, kernel, bias=None, **kwargs):\n",
    "    x, kernel = causal_padding(x, kernel)\n",
    "    return F.conv1d(x, kernel, bias=bias, padding=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mgetattr\u001b[39m(torch, window)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'window' is not defined"
     ]
    }
   ],
   "source": [
    "torch.      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0041225012c46a81f6ea3650572d975902102d8f41a1704402cdfe5d667efe52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
